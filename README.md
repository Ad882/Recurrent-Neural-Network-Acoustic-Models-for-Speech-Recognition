<h1 align='center'> Recurrent Neural Network Acoustic Models for Speech Recognition ğŸ—£ï¸ </h1>

This project explores the use of Recurrent Neural Networks (RNNs), specifically Long Short-Term Memory (LSTM) networks, for speech recognition tasks. The project is inspired by a Google [paper](paper.pdf) on speech recognition.    
The main goal of the Notebook is to serve as an educational resource for learning how speech-to-text systems work, from the features extraction (with the use of Mel-frequency cepstral coefficients (MFCCs) and transcription encoding), the construction of the model, the use of the Connectionist Temporal Classification (CTC) loss, the Word Error Rate (WER)...   
Watch the project [poster](poster.pdf)!   
Finally, an ASR application is in progress... ğŸš§

<br>

## ğŸŒŸ Features

In this notebook, we implement a speech recognition system using LSTM-based RNNs with CTC loss. The notebook is designed to be educational and will walk through several important concepts in speech processing and deep learning, such as:

- **Mel-Frequency Cepstral Coefficients (MFCCs):** Used as a feature representation for speech signals.
- **LSTM-based RNNs:** Using Long Short-Term Memory networks for sequence modeling.
- **CTC Loss (Connectionist Temporal Classification):** A loss function that enables speech-to-text models to align sequences of different lengths (speech and text).

The project is structured to help understanding the theory behind these concepts, as well as how they are implemented in practice for speech recognition tasks.

<br>

An ASR application as been added based on the [open-ai whisper](https://openai.com/index/whisper/) model. 

<br>

## ğŸ—‚ï¸ Project Structure

Here's the current structure of the project:

```
Recurrent-Neural-Network-Acoustic-Models-for-Speech-Recognition/
â”œâ”€â”€ app/                                # Raw and processed audio data 
â”‚   â”œâ”€â”€ devices.py                      # Check available microphones 
â”‚   â”œâ”€â”€ load.py                         # Whisper model loading 
â”‚   â”œâ”€â”€ main.py                         # Main ASR Application 
â”‚   â””â”€â”€ microphone_test.py              # Test microphone 
â”‚   
â”œâ”€â”€ data/                               # Raw and processed audio data 
â”‚   â”œâ”€â”€ audio_sample                    # Raw audio used in the Notebook 
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚ 
â”‚   â”œâ”€â”€ test                            # Numpy array processed test audio files
â”‚   â”‚   â”œâ”€â”€ X_test_augmented.npy
â”‚   â”‚   â””â”€â”€ X_test_augmented.npy
â”‚   â”‚ 
â”‚   â”œâ”€â”€ train                           # Numpy array processed train audio files
â”‚   â”‚   â”œâ”€â”€ X_train_augmented.npy
â”‚   â”‚   â””â”€â”€ X_train_augmented.npy
â”‚   â”‚ 
â”‚   â”œâ”€â”€ test_validated.json             # Test file contenaing data path and transcript
â”‚   â””â”€â”€ train_validated.json            # Train file contenaing data path and transcript
â”‚
â”œâ”€â”€ img/                                # Notebook images
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ model/                              # Trained models 
â”‚   â”œâ”€â”€ model_weights_augmented.pth     # 'Final' RNN LSTM model
â”‚   â””â”€â”€ single.pth                      # Model trained with one audio file
â”‚
â”œâ”€â”€ .gitignore                          # Git ignore file
â”œâ”€â”€ environment.yaml                    # Yaml file to create the environment
â”œâ”€â”€ LICENSE                             # License file
â”œâ”€â”€ notebook.ipynb                      # Educative notebook
â”œâ”€â”€ paper.pdf                           # Paper that inspired the repo
â”œâ”€â”€ poster.pdf                          # Project poster
â”œâ”€â”€ README.md                           # Project documentation (this file)
â””â”€â”€ requirements.txt                    # Python dependencies
```


<br>

## ğŸ’¾ Dataset

The dataset used for this project is the [Mozilla Common Voice](https://commonvoice.mozilla.org/en/datasets) dataset. It is not in the [data](data) folder, as it is too heavy!

<br>

## âš¡ Quick Start

Before you can simulate the attack, you need to set up the project and configure the environment variables.


### 1. Clone the Repository ğŸ“¥

```bash
git clone https://github.com/Ad882/Recurrent-Neural-Network-Acoustic-Models-for-Speech-Recognition.git
cd Recurrent-Neural-Network-Acoustic-Models-for-Speech-Recognition
```

--- 
### 2. Install Dependencies ğŸ§‘â€ğŸ’»

Make sure you have Python 3.7+ installed. Then, install the necessary dependencies with:

```bash
pip install -r requirements.txt
```

It is possible to create a new environment with all the dependancies:

```bash
conda env create --name asr --file=environment.yaml
```
  
--- 
### 4. Run the Notebook ğŸš€

Then just run the notebook and enjoy!


<br>


## ğŸ™ï¸ Application

An ASR application is being implemented...
