{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition üó£Ô∏è </h1>\n",
    "\n",
    "<h2 align='center'> - </h2>\n",
    "\n",
    "<h3 align='center'> Techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition </h3>\n",
    "\n",
    "This notebook is directly inspired by the paper [Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition](https://arxiv.org/abs/1507.06947) released in July 2015. The latter will be refered as 'the paper' during the whole notebook.\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Introduction to Speech Recognition**  \n",
    "Speech Recognition, or Automatic Speech Recognition (**ASR**), refers to the process of converting spoken language into text using computational models. Unlike **voice recognition**, which aims to identify the speaker based on their unique vocal characteristics, or **emotion recognition**, which detects the emotional state of a speaker, ASR focuses purely on transcribing the linguistic content of speech. Modern ASR systems leverage deep learning models, particularly Recurrent Neural Networks (RNNs) and Transformer-based architectures, to achieve high accuracy in various acoustic environments.\n",
    "\n",
    "Over time, several methods have been developed to tackle speech recognition, such as:  \n",
    "- Gaussian Mixture Models (GMMs),  \n",
    "- Hidden Markov Models (HMMs),  \n",
    "- Deep Neural Networks (DNNs).  \n",
    "\n",
    "However, because speech is a naturally occurring time sequence, it requires a neural network capable of processing sequential data effectively. Additionally, the system must be lightweight in terms of memory and computational requirements, as the goal is to achieve real-time speech recognition. **Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, are well-suited for this task.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I. Representing speech signals**  \n",
    "A speech signal is a continuous waveform that conveys information through variations in air pressure.   \n",
    "Let's take an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf                                          \n",
    "\n",
    "path = 'data/audio_sample/84-121123-0000.flac'                                                  \n",
    "waveform, sample_rate = sf.read(path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "audio = Audio(waveform, rate=sample_rate)\n",
    "display(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To process it computationally, there are several meaningful representations:\n",
    "\n",
    "1. **Raw waveform**: The most basic form of representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(22, 8))\n",
    "plt.plot(waveform)\n",
    "plt.title(\"Audio signal waveform\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it is not directly suitable for ASR due to:\n",
    "- **High dimensionality**: A typical speech signal sampled at 16 kHz produces 16,000 values per second, making direct processing computationally expensive.  \n",
    "- **Speaker and noise variability**: Raw waveforms are highly sensitive to variations in speaking style, microphone quality, and background noise... \n",
    "- **Lack of structured information**: ASR models struggle to extract phonetic and linguistic patterns directly from waveforms due to the complexity of raw acoustic signals. \n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Fourier transform**: Converts the signal into a sum of sinusoidal components, revealing spectral content over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = len(waveform)\n",
    "T = 1.0 / sample_rate\n",
    "yf = np.fft.fft(waveform)\n",
    "xf = np.fft.fftfreq(n, T)[:n//2]\n",
    "\n",
    "amplitude = 2.0/n * np.abs(yf[:n//2])\n",
    "\n",
    "plt.figure(figsize=(22, 8))\n",
    "plt.plot(xf, amplitude)\n",
    "plt.title('Audio signal FFT')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFT is also not very suitable for ASR because:   \n",
    "- **Loss of temporal information**: The FFT gives a static frequency spectrum but does not show how frequencies change over time, which is crucial for speech understanding.  \n",
    "- **Too much unnecessary detail**: The FFT produces a very detailed frequency representation, but not all frequencies contribute equally to speech perception. The human ear, for instance, perceives frequencies on a non-linear scale (Mel scale).  \n",
    "- **No emphasis on speech-specific features**: It does not highlight the phonetic structures and speech-relevant frequency bands needed for ASR models. \n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Spectrogram**: A time-frequency representation showing how signal energy is distributed across different frequencies over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(waveform)), ref=np.max)\n",
    "plt.figure(figsize=(22, 8))\n",
    "librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log', cmap='magma')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Audio signal spectrogram (dB)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectogram will not be used for ASR because:\n",
    "- **High redundancy**: Spectrograms contain a lot of information that is not necessary for recognizing phonemes, such as harmonics unrelated to speech content.  \n",
    "- **Linear frequency scale**: Standard spectrograms use a linear frequency scale, while human perception is more logarithmic, making them less efficient for speech feature extraction.  \n",
    "- **Sensitivity to noise and variability**: While spectrograms capture useful phonetic structures, they can be affected by background noise and variations in pronunciation, making direct usage less optimal.  \n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Mel spectrogram**: A spectrogram processed with the Mel scale, which mimics human auditory perception by emphasizing perceptually relevant frequency bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=128)\n",
    "mel_spectrogram_db = librosa.amplitude_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(22, 8))\n",
    "librosa.display.specshow(mel_spectrogram_db, sr=sample_rate, x_axis='time', y_axis='mel', cmap='magma')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Audio signal Mel spectrogram (dB)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mel spectrogram** is a better alternative to spectrogram because it applies a mel scale transformation to mimic human auditory perception. The mel scale is a **perceptual** scale of pitches judged by listeners to be equal in distance from one another. A formula ([O'Shaughnessy](https://en.wikipedia.org/wiki/Daniel_O%27Shaughnessy) 1987) to convert f (Hz) into m (mels) is:\n",
    "\n",
    "$$ m = 2595 \\times log_{10}(1 + \\frac{f}{700})$$\n",
    "\n",
    "\n",
    "\n",
    "Looking at the spectrogram and the mel spectrogram, it is remarkable that the mel spectrogram contains **less information** than the classic spectrogram, because it is designed to mimic human perception rather than capturing all the raw frequencies. Thus, high frequencies are compressed and the number of frequency bands is reduced.\n",
    "\n",
    "\n",
    "\n",
    "That is why Mel-Frequency Cepstral Coefficients (MFCCs) are widely used as input features for ASR models. They are derived from the Mel spectrogram and capture both spectral and cepstral (rate of spectral change) characteristics of speech. The main reasons for using MFCCs in ASR are:\n",
    "\n",
    "- **Auditory inspiration**: MFCCs approximate the non-linear frequency perception of the human ear, **focusing on lower frequencies where speech information is most concentrated.**\n",
    "- **Dimensionality reduction**: Instead of using raw waveforms or high-dimensional spectrograms, MFCCs provide a compact yet informative representation of speech.\n",
    "- **Robustness**: They enhance ASR performance by filtering out non-linguistic variations and making the system more invariant to speaker differences and environmental noise.\n",
    "\n",
    "More information about Mel spectrogram [here](https://importchris.medium.com/how-to-create-understand-mel-spectrograms-ff7634991056).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Thus, in this notebook, the inputs will be the MFCCs extracted from short audio signals and the outputs the corresponding transcripts for each audio signal.  The dataset was taken from the [Mozilla Common Voice dataset](https://commonvoice.mozilla.org/en/datasets).    \n",
    "\n",
    "\n",
    "<h3 align='center'>\n",
    "    <img src=\"img/Common_Voice_Banner2.png\">\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **II. Features processing**  \n",
    "\n",
    "### **1. Data augmentation**\n",
    "Before extracting the MFCCs, it is required to do some *data augmentation* to improve the model. In fact, audio data augmentation is the process of applying transformations to audio data to artificially increase the size and diversity of a dataset. This helps improve the **robustness** and **generalization**.    \n",
    "\n",
    "Here are some widely used augmentation methods:\n",
    "\n",
    "| **Technique**        | **Description** | **Effect on Model** |\n",
    "|---------------------|----------------|---------------------|\n",
    "| **Add Noise**        | Add white noise, background noise, or other disturbances | Helps model generalize to noisy environments |\n",
    "| **Time Stretching**  | Speeds up or slows down the audio without changing pitch | Mimics variations in speech rate |\n",
    "| **Pitch Shifting**   | Shifts the pitch up or down | Simulates different voices or instruments |\n",
    "| **Time Shifting**    | Moves the audio forward or backward in time | Adds temporal variations |\n",
    "| **Volume Scaling**   | Increases or decreases loudness | Makes the model robust to volume changes |\n",
    "| **Reverberation**    | Adds echo or room effects | Simulates different acoustic environments |\n",
    "| **Bandpass Filtering** | Removes or enhances specific frequency ranges | Can simulate microphone effects |   \n",
    "\n",
    "In the paper, data augmentation was also performed on input features:\n",
    ">To achieve robustness to background noise and reverberant environments we synthetically distort each utterance in a room simulator with a virtual noise source. Noise is taken from the audio of [YouTube](https://www.youtube.com/) videos. Each utterance is randomly distorted to get 20 variations. This ‚ÄúMulti-style training‚Äù also alleviates overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(audio, noise_level=0.005):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    return np.clip(audio + noise_level * noise, -1.0, 1.0)\n",
    "\n",
    "def shift_audio(audio, shift_max=0.2):\n",
    "    shift = int(np.random.uniform(-shift_max, shift_max) * len(audio))\n",
    "    return np.roll(audio, shift)\n",
    "\n",
    "def time_stretch(audio, rate=1.0):\n",
    "    return librosa.effects.time_stretch(audio, rate=rate)\n",
    "\n",
    "def pitch_shift(audio, sr, n_steps_range=(-3, 3)):\n",
    "    n_steps = np.random.randint(n_steps_range[0], n_steps_range[1] + 1)\n",
    "    return librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=n_steps)\n",
    "\n",
    "def apply_augmentations(audio, sr, noise_prob=0.8, shift_prob=0.5, stretch_prob=0.5, pitch_prob=0.5):\n",
    "    \n",
    "    if np.random.rand() < noise_prob:\n",
    "        audio = add_noise(audio)\n",
    "    \n",
    "    if np.random.rand() < shift_prob:\n",
    "        audio = shift_audio(audio)\n",
    "    \n",
    "    if np.random.rand() < stretch_prob:\n",
    "        audio = time_stretch(audio)\n",
    "    \n",
    "    if np.random.rand() < pitch_prob:\n",
    "        audio = pitch_shift(audio, sr)\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = apply_augmentations(waveform, sample_rate)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "librosa.display.waveshow(waveform, sr=sample_rate)\n",
    "plt.title(\"Original audio waveform\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "librosa.display.waveshow(augmented_audio, sr=sample_rate)\n",
    "plt.title(\"Augmented audio waveform\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Extracting MFCCs from audio signals**  \n",
    "\n",
    "The extraction of the **Mel-Frequency Cepstral Coefficients (MFCCs)** from audio is described in the paper:   \n",
    ">We use 80-dimensional log mel filterbank energy features computed every 10ms on 25ms windows.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Here are the detailed steps for the extraction from the audio signal $x(t)$:\n",
    "\n",
    "#### **a) Framing the Signal**  \n",
    "- The signal $x(t)$ is sampled at a rate $ f_s = 16 \\text{ kHz}$, resulting in the discrete-time signal $ x[n] = x(nT_s) $, where $ T_s = \\frac{1}{f_s} $ is the sampling period.  \n",
    "- The signal is divided into **25ms** frames with a **10ms** stride. The corresponding number of samples per frame and hop size are given by:  \n",
    "  $$N = \\text{round}(25 \\times 10^{-3} \\cdot f_s)$$\n",
    "  $$H = \\text{round}(10 \\times 10^{-3} \\cdot f_s)$$ \n",
    "- A window function $ w(n) $ (e.g., [**Hamming window**](https://en.wikipedia.org/wiki/Window_function#Hann_and_Hamming_windows)) is applied to minimize discontinuities at the frame edges.  \n",
    "- The windowed signal for frame $ m $ is given by:  \n",
    "  $$x_w[m, n] = x[n + mH] \\cdot w[n], \\quad 0 \\leq n < N, \\quad m = 0, 1, 2, \\dots$$ \n",
    "\n",
    "---\n",
    "\n",
    "#### **b) Fourier Transform (FFT)**  \n",
    "The **Fast Fourier Transform (FFT)** is applied to obtain the frequency spectrum $X(f)$.  \n",
    "  $$X(f) = \\sum_{n=0}^{N-1} x_w(n) e^{-j 2\\pi f n / N}$$\n",
    "\n",
    "where, $f_n = \\frac{n f_s}{N}, \\quad n = 0, 1, ..., N-1$\n",
    "\n",
    "\n",
    "The **power spectrum**, $P(f)$, represents the energy at each frequency bin $f$ and can be computed as: $$P(f) = |X(f)|^2$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **c) Applying the Mel filterbank**  \n",
    "The **Mel filter bank** chosen consists of a set of overlapping **80 triangular filters** $H_m(f)$, designed to mimic human auditory perception. Each filter emphasizes a specific range of frequencies in the **Mel scale**.\n",
    "\n",
    "The **filtered energy** for the $m$-th Mel band is computed as:\n",
    "\n",
    "$$S_{\\text{mel}}(m) = \\sum_{f=0}^{F-1} H_m(f) P(f)$$\n",
    "\n",
    "where:\n",
    "- $H_m(f)$ is the weight of the **m-th Mel filter** at frequency bin $f$, forming a set of overlapping triangular filters.\n",
    "- $P(f)$ is the **power spectrum** obtained from the FFT.\n",
    "- $S_{\\text{mel}}(m)$ is the **filtered energy** corresponding to the $m$-th Mel frequency band.\n",
    "\n",
    "\n",
    "<h3 align='center'>\n",
    "    <img src=\"img/mel_bank_filter.png\">\n",
    "</h3>\n",
    "\n",
    "\n",
    "\n",
    "#### **d) Logarithm of filtered energy**  \n",
    "Since human perception of loudness is logarithmic, a **logarithm** is applied to the Mel-scaled spectrum:\n",
    "\n",
    "$$E_m = \\log S_{\\text{mel}}(m) = \\log \\sum_{f=0}^{F-1} H_m(f) P(f)$$\n",
    "\n",
    "where:\n",
    "- $E_m$ is the **logarithmic Mel energy**.\n",
    "- $H_m(f)$ is the response of the $m$-th Mel filter.\n",
    "\n",
    "#### **e) Discrete Cosine Transform (DCT)**  \n",
    "Finally, a **Discrete Cosine Transform (DCT)** is applied to the **log Mel energy vector** to obtain the **Mel-Frequency Cepstral Coefficients (MFCCs)**:\n",
    "\n",
    "$$C_n = \\sum_{m=0}^{M-1} E_m \\cos \\left( \\frac{\\pi n (2m+1)}{2M} \\right)$$\n",
    "\n",
    "where:\n",
    "- $C_n$ is the $n$-th **MFCC coefficient**.\n",
    "- $M$ is the total number of Mel filter banks.\n",
    "\n",
    "<br>\n",
    "\n",
    "The [librosa](https://librosa.org/doc/latest/index.html) library computes the extraction easily!   \n",
    "*Get more information about MFCCs in this [paper](https://ieeexplore.ieee.org/document/9955539).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio, n_mels=80, frame_length=0.025, frame_stride=0.01, sr=16000):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=n_mels)\n",
    "    log_mel = librosa.power_to_db(mel_spec)\n",
    "    return log_mel.T  # (time, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a frame 'stacking' and 'decimation' is performed as described in the paper:\n",
    "\n",
    ">One way of reducing the huge space of alignments is to reduce the number of input frames. This can be done by simply decimating the input frames, though to present the full acoustic information of the input signal, we first stack frames so that the networks sees multiple (e.g. 8) frames at a time but then decimate the frames so that we skip forward multiple frames (e.g. 3) after processing each such ‚Äúsuper-frame‚Äù. [...]   \n",
    "By decimating the frames in this manner, the acoustic model is able to process the full signal but acoustic model computation need only happen every 30ms. For a network of a fixed size this results in a dramatic reduction in the acoustic model computation and decoding time.\n",
    "\n",
    "<br>\n",
    "\n",
    "It is useful for ASR because:\n",
    "\n",
    "- **Computational efficiency**: By reducing the number of frames processed, the acoustic model runs faster, reducing both **training time** and **real-time inference cost**.\n",
    "\n",
    "- **Maintaining full acoustic information**: Even though we skip frames, stacking multiple frames before skipping ensures that the model still has access to a **rich temporal context**.  \n",
    "\n",
    "- **Better generalization**: Since ASR models rely on capturing phonetic patterns over time, stacking frames allows them to recognize speech structures more effectively, rather than making predictions based on a single, noisy frame.  \n",
    "\n",
    "\n",
    "<h3 align='center'>\n",
    "    <img src=\"img/stacking.png\">\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_decimate_frames(features, stack_size=3, skip=3):\n",
    "    stacked_features = []\n",
    "    for i in range(0, len(features) - stack_size + 1, skip):\n",
    "        stacked_features.append(np.concatenate(features[i:i+stack_size], axis=0))\n",
    "    return np.array(stacked_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make an example of augmentation, MFCCs extraction and stacking and decimating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = apply_augmentations(waveform, sample_rate)\n",
    "extracted_features = extract_features(augmented_audio)\n",
    "print(f\"extracted_features:\\n{extracted_features}\")\n",
    "print(f\"size extracted_features:{extracted_features.shape}\\n\")\n",
    "\n",
    "stacked_extracted_features = stack_decimate_frames(extracted_features)\n",
    "print(f\"stacked_extracted_features:\\n{stacked_extracted_features}\")\n",
    "print(f\"size stacked_extracted_features:{stacked_extracted_features.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this example, the feature output size is **(22, 240)**:\n",
    "\n",
    "#### **22: Number of frames after decimation**   \n",
    "The original audio produced $N = 66$ **MFCCs frames** before stacking (dimension 0 of the `extracted_features` vector). After **decimation**, with `skip=3`, the number of remaining frames becomes **N' ‚âà N / 3**. The **first dimension** (22) represents then the final number of **super-frames** processed after stacking and decimation.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **240: Size of the feature vector**      \n",
    "Before stacking, each frame had **80 MFCC coefficients** (because of the bank of 80 Mel filters). With `stack_size=3`, **3 frames** are stacked to form a **super-frame**. The concatenation of the MFCC vectors gives $80 \\times 3 = 240$. The **second dimension** 240 corresponds to the **length of the feature vector per super-frame**.\n",
    "\n",
    "\n",
    "*Note: The first dimension can vary, depending on the length of the audio, while the second one will always be 240 in this case.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Encoding audio transcripts**   \n",
    "Now that the input data is processed, it is necessary to take care of the target data: the **audio transcripts**. An audio transcript is the written representation of the spoken content in an audio signal.    \n",
    "It is not possible to pass the sentences themselves as targets. Thus, the question of how to represent the spoken content raises: Represent the transcript **letter by letter**, **word by word**, or **phonematically** (using phonemes)?\n",
    "\n",
    "<br>\n",
    "\n",
    "- #### **Letter-by-letter representation**  \n",
    "    Representing speech transcription letter by letter is not ideal for several reasons:\n",
    "    - **Context dependency**: Letters in words can have different pronunciations based on their context. For example, the letter \"c\" can be pronounced as /k/ in \"cat\" but as /s/ in \"cent.\" This variation makes it challenging to model letters directly in speech recognition.\n",
    "    - **Inefficiency**: Since the ASR model would need to account for all possible variations of each letter in different contexts, this representation would make the training process more complex and less efficient.   \n",
    "    *Note: Some well-known and effective models are based on predicting letters. (cf some of these [torch models](https://pytorch.org/audio/main/models.html))*   \n",
    "\n",
    "<br>\n",
    "\n",
    "- #### **Word-by-word representation**  \n",
    "    Representing transcripts word by word also presents challenges:\n",
    "    - **Vocabulary size**: A word-level representation would require a very large dictionary to cover the many thousands of words in any natural language. This can lead to an enormous model size and complicate the recognition process.\n",
    "    - **Out-of-vocabulary words**: ASR models would struggle with new words or names that are not part of the fixed vocabulary, resulting in inaccurate or failed transcriptions.\n",
    "  \n",
    "<br>\n",
    "\n",
    "- #### **Phonetic representation**  \n",
    "    The most effective approach in ASR is to represent the transcript **phonematically**, using a **phoneme-based** transcription. Phonemes are the smallest distinct units of sound in a language. In English, for example, the word \"cat\" can be broken down into its phonemes: /k/, /√¶/, and /t/. This approach offers several advantages:\n",
    "    - **Pronunciation variability**: Phonemes capture the actual sounds of speech, irrespective of the letters used to spell the words. This allows the model to account for how words are pronounced differently in various contexts.\n",
    "    - **Compact and flexible**: Phoneme-based systems reduce the vocabulary size since they only need to recognize a set of phonemes (e.g., around 40 phonemes in English), as opposed to the vast number of possible words. This makes the ASR model more manageable and flexible.\n",
    "    - **Language agnostic**: Phonemes are language-independent and can be used for any spoken language, allowing the system to generalize across accents and dialects.\n",
    "\n",
    "<br>\n",
    "\n",
    "So now the next steps are to:   \n",
    "- Convert all the transcripts into their phoneme sequences\n",
    "- Build a dictionary of phonemes\n",
    "- Encode each phoneme as an integer (for the model)\n",
    "\n",
    "\n",
    "#### a) Convert all the transcripts into their phoneme sequences\n",
    "This will be realized by the [g2p](https://github.com/Kyubyong/g2p) library, which is a module for English grapheme to phoneme conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from g2p_en import G2p\n",
    "g2p = G2p()\n",
    "\n",
    "\n",
    "def text_to_phonemes(text):\n",
    "    # Clean up text: remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    phonemes = g2p(text)\n",
    "\n",
    "    # Take the first pronunciation and remove the numbers indicating stress (e.g. AH1 ‚Üí AH)\n",
    "    phonemes = [p.rstrip('012') for p in phonemes]\n",
    "\n",
    "    # Replace the blank space ' ' ‚Üí '-'\n",
    "    phonemes = ['-' if item == ' ' else item for item in phonemes]\n",
    "    return phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Build a dictionary of phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of authorized ARPAbet phonemes (40 English phonemes)\n",
    "PHONEMES = [\n",
    "    \"IY\", \"IH\", \"EH\", \"AE\", \"AA\", \"AH\", \"AO\", \"UH\", \"UW\", \"ER\", \"AX\", \"EY\", \"AY\", \"OW\", \"OY\", \"AW\",\n",
    "    \"P\", \"B\", \"T\", \"D\", \"K\", \"G\", \"CH\", \"JH\", \"F\", \"V\", \"TH\", \"DH\", \"S\", \"Z\", \"SH\", \"ZH\",\n",
    "    \"HH\", \"M\", \"N\", \"NG\", \"L\", \"R\", \"W\", \"Y\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Encode each phoneme as an integer (for the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary building (phoneme mapping ‚Üí index)\n",
    "vocab = {phoneme: (idx + 2) for idx, phoneme in enumerate(PHONEMES)}\n",
    "\n",
    "# Adding blank space '-'\n",
    "vocab['-'] = 1\n",
    "\n",
    "\n",
    "def encode_phonemes(text, vocab):\n",
    "    phoneme_list = text_to_phonemes(text)\n",
    "    encoded_sequence = [vocab.get(p) for p in phoneme_list]\n",
    "    \n",
    "    return encoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, another target is defined:\n",
    ">The targets [...] can be efficiently and easily computed using **finite state transducers** (FSTs) [...] with additional optional blank states interposed between the states of the sequence labels.\n",
    "\n",
    "A **Finite-State Transducer (FST)** is a type of **finite-state machine** that maps input sequences to output sequences. In **speech recognition**, FSTs are widely used to model the relationships between different levels of linguistic representation:\n",
    "\n",
    "<h3 align='center'>\n",
    "    <img src=\"img/fst.jpeg\">\n",
    "</h3>\n",
    "\n",
    "FSTs will not be used in this notebook as it will increase the complexity, but you can find more information [here](https://ieeexplore.ieee.org/document/7178778).\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's now take an example with a \"difficult\" sentence:   \n",
    "\n",
    "<h4 align='center'>\n",
    "    'Tymoshenko won the gold medal in all around ahead of Skaldina.'\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"Tymoshenko won the gold medal in all around ahead of Skaldina.\"\n",
    "encoded_phonemes = encode_phonemes(transcript, vocab)\n",
    "print(f\"Transcript: {transcript}\")\n",
    "print(f\"Phonemes: {text_to_phonemes(transcript)}\")\n",
    "print(f\"Encoded phonemes: {encoded_phonemes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the input data (MFCC features) and targets (encoded phonemes) are defined.   \n",
    "\n",
    "However, some processing is still required to manage the total volume of data.   \n",
    "First, the input data must be **normalized**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mfccs(mfccs):\n",
    "    mean = np.mean(mfccs, axis=0, keepdims=True)\n",
    "    std = np.std(mfccs, axis=0, keepdims=True)\n",
    "\n",
    "    normalized_mfccs = (mfccs - mean) / (std + 1e-6)\n",
    "\n",
    "    return normalized_mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a function that handles the extraction of input data and the encoding of targets was defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_and_text(entry, vocab, X, Y):\n",
    "    # Load audio features\n",
    "    y, sr = librosa.load(entry[\"key\"], sr=16000)\n",
    "\n",
    "    # If no features, ignore this entry\n",
    "    if y is None:\n",
    "        print(f\"Skipping entry due to missing or invalid audio file: {entry['key']}\")\n",
    "        return # Exit the function for this `entry`\n",
    "\n",
    "    # Aplly random data augmentation\n",
    "    augmented_audio = apply_augmentations(y, sr)\n",
    "\n",
    "    # Extract audio features\n",
    "    features = extract_features(augmented_audio)\n",
    "\n",
    "    # Stack and downsample\n",
    "    features = stack_decimate_frames(features)\n",
    "    features = normalize_mfccs(features)\n",
    "\n",
    "    # Convert text to indices\n",
    "    encoded_phonemes = encode_phonemes(entry[\"text\"], vocab)\n",
    "\n",
    "\n",
    "    # The number of audio feature frames must be greater than or equal to the number of phoneme labels...\n",
    "    if (features.shape[0]) < (len(encoded_phonemes)):\n",
    "        print(f\"Skipping entry otherwise CTC will be infinite...\")\n",
    "        return \n",
    "    else:\n",
    "        X.append(features)\n",
    "        Y.append(encoded_phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code processes all the data in the training dataset and stores the normalised MFCCs features in the X_train variable and the associated transcripts in the y_train variable.   \n",
    "\n",
    "This code takes a few minutes to run, so it will not be run here, X_train and y_train have been saved. \n",
    "\n",
    "\n",
    "```python\n",
    "    import json\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    train_file_path = 'data/train_validated.json'\n",
    "    with open(train_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "    # Apply preprocess on each sample\n",
    "    for entry in train_data:\n",
    "    process_audio_and_text(entry, vocab, X_train, y_train)\n",
    "\n",
    "    # Adding padding to standardize sizes\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    X_train_padded = pad_sequences(X_train, padding='post', value=0.0, dtype='float16')\n",
    "    y_train_padded = pad_sequences(y_train, padding='post', value=99, dtype='int32')\n",
    "    X_train = np.array(X_train_padded)\n",
    "    y_train = np.array(y_train_padded)\n",
    "\n",
    "    np.save('data/train/X_train.npy', X_train)\n",
    "    np.save('data/train/y_train.npy', y_train)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Audio recordings and their corresponding transcripts vary in length. However, deep learning models typically require fixed-size inputs for efficient batch processing. To accommodate this, **padding** was applied to both inputs and targets, adding extra tokens (usually a special padding symbol) to the end of shorter sequences so that all sequences in a batch have the same length.   \n",
    "*Note: For the input the padding value is 0.0, which corresponds to no signal and for the targets the value 99 was chosen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['<PAD>'] = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/train/X_train_augmented.npy')\n",
    "y_train = np.load('data/train/y_train_augmented.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)  # (num_samples, max_time_steps, n_features)\n",
    "print(\"Shape of y_train:\", y_train.shape)  # (num_samples, max_seq_length)\n",
    "\n",
    "print(\"Sample X_train:\", X_train[0][:5])  # Displaying the first frames\n",
    "print(\"Sample y_train:\", y_train[0])  # Displaying the encoded transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\", align=\"center\">\n",
    "\n",
    "**Exercise:** Can you write a function that decodes the the encoded transcript `y_train[0]` into phonemes / text ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same code is applied for the test dataset:\n",
    "\n",
    "```python\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    test_file_path = 'data/test_validated.json'\n",
    "\n",
    "    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        test_data = [json.loads(line) for line in f]\n",
    "\n",
    "    for entry in test_data:\n",
    "        process_audio_and_text(entry, vocab, X_test, y_test)\n",
    "\n",
    "\n",
    "    X_test_padded = pad_sequences(X_test, padding='post', maxlen=X_train.shape[1], value=0.0, dtype='float16')\n",
    "    y_test_padded = pad_sequences(y_test, padding='post', maxlen=y_train.shape[1], value=99, dtype='int32')\n",
    "    X_test = np.array(X_test_padded)\n",
    "    y_test = np.array(y_test_padded)\n",
    "\n",
    "\n",
    "    np.save('data/test/X_test.npy', X_test)\n",
    "    np.save('data/test/y_test.npy', y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('data/test/X_test_augmented.npy')\n",
    "y_test = np.load('data/test/y_test_augmented.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Model definition\n",
    "\n",
    "\n",
    "Now that the inputs (**MFCC features**) and targets (encoded transcripts) are processed, let's proceed with constructing the ASR model. As described in the paper, the model is a **5-layer LSTM-based Recurrent Neural Network (RNN)**. Each LSTM layer is **bidirectional**, meaning it processes the input sequence in both forward and backward directions, capturing long-range dependencies in speech. Additionally, each layer is connected to both the previous **forward and backward layers**, ensuring effective information flow across time steps. Finally, the output layer receives connections from both the final forward and backward LSTM layers, allowing the model to generate accurate predictions by leveraging contextual information from both past and future frames. Each LSTM layer consists of **300 neurons (hidden units)** per direction. This means that at each time step, both the forward and backward LSTMs maintain a **300-dimensional hidden state**, resulting in a total hidden representation of **600 dimensions** per layer after concatenation.   \n",
    "This architecture is well-suited for ASR, as it enables the model to capture phonetic structures while maintaining temporal consistency in speech transcription:\n",
    "\n",
    "<h3 align='center'>\n",
    "    <img src=\"img/model.png\">\n",
    "</h3>\n",
    "\n",
    "\n",
    "For a **bidirectional LSTM**, both forward and backward contexts are incorporated by using two hidden states:  \n",
    "- $\\overrightarrow{h_t}$ (forward LSTM processing from left to right)\n",
    "- $\\overleftarrow{h_t}$ (backward LSTM processing from right to left)\n",
    "\n",
    "Each hidden state is computed as:\n",
    "\n",
    "$$\\overrightarrow{h_t} = f(W_x \\overrightarrow{x_t} + W_h \\overrightarrow{h_{t-1}} + b)$$\n",
    "\n",
    "$$\\overleftarrow{h_t} = f(W_x \\overleftarrow{x_t} + W_h \\overleftarrow{h_{t+1}} + b)$$\n",
    "\n",
    "The final hidden representation at each time step is obtained by concatenating the forward and backward hidden states:\n",
    "\n",
    "$$h_t = [\\overrightarrow{h_t} ; \\overleftarrow{h_t}]$$\n",
    "\n",
    "The output layer receives this combined hidden state and estimates the label posteriors as:\n",
    "\n",
    "$$p(l_t | x_t, h_t) = p(l_t | x_t, \\overrightarrow{h_t}, \\overleftarrow{h_t})$$\n",
    "\n",
    "\n",
    "where: \n",
    "- $x_t$ is the input vector at time step $t$.\n",
    "- $f$ is the activation function (usually a combination of sigmoid and tanh).\n",
    "- $W_x$ is the input weight matrix, that transforms the input $x_t$ into the LSTM's hidden space.   \n",
    "- $W_h$ is the recurrent weight matrix, that determines how the hidden state from the previous time step ($h_{t-1}$) influences the current hidden state.  \n",
    "- $b$ is a bias vector that allows the model to learn an offset for the activations.  \n",
    "- $l_t$ is the posterior probability of the label given the input sequence and the hidden state.\n",
    "\n",
    "\n",
    "*Note: The input dimension is 240 (the size of the feature vector seen earlier) and the output dimension is 42, the size of the vocabulary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_dim = 240\n",
    "hidden_dim = 300\n",
    "num_layers = 5\n",
    "num_classes = len(vocab) + 1\n",
    "bidirectional = True\n",
    "dropout=0.5\n",
    "     \n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, dropout=dropout):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    # Weight initialization\n",
    "    # def init_weights(self):\n",
    "    #     for name, param in self.named_parameters():\n",
    "    #         if 'weight' in name:\n",
    "    #             nn.init.uniform_(param, -0.04, 0.04)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        num_directions = 2 if self.lstm.bidirectional else 1\n",
    "        return (torch.zeros(num_layers * num_directions, batch_size, hidden_dim),\n",
    "                torch.zeros(num_layers * num_directions, batch_size, hidden_dim))\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(x, input_lengths.to(device), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Memory cell activation limited to [-50, 50].\n",
    "        hn = torch.clamp(hn, -50, 50)\n",
    "        cn = torch.clamp(cn, -50, 50)\n",
    "\n",
    "\n",
    "        output = self.fc(output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = SpeechRecognitionModel(input_dim, hidden_dim, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, it is explicitly written how they initialized the model's weights:      \n",
    ">The weights in all the networks are randomly initialized with a uniform (-0.04, 0.04) distribution.    \n",
    "\n",
    "This proved to be a very poor initialization for the model, as it always converged to empty outputs. The input tensors were then filled with values using a [Xavier uniform distribution](https://pytorch.org/docs/stable/nn.init.html).\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "In the paper, it is also wrritten that:   \n",
    ">We clip the activations of memory cells to [-50,50].\n",
    "\n",
    "<br>\n",
    "\n",
    "In addition, a mask has been added to ignore the 0 padding contained in the entries, thanks to the function [pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html).   \n",
    "\n",
    "<br>\n",
    "\n",
    "Let's try to train the model with a single audio, here its the transcript: \n",
    "\n",
    "<h4 align='center'>\n",
    "    'Jesus loves me, he who died.'\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_test = 'data/audio_sample/common_voice_en_39177958.mp3' \n",
    "transcript = 'Jesus loves me, he who died.'\n",
    "\n",
    "# Extract input features\n",
    "y, sr = librosa.load(audio_test, sr=16000)\n",
    "features = normalize_mfccs(stack_decimate_frames(extract_features(y)))\n",
    "input = torch.as_tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "input_length = torch.tensor([input.shape[1]], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Encode target\n",
    "target = np.array(encode_phonemes(transcript, vocab))\n",
    "target_length = torch.tensor([len(target)], dtype=torch.long)\n",
    "target = torch.tensor(target, dtype=torch.int32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"input: {input}\\n\")\n",
    "print(f\"target: {target}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input, input_length)\n",
    "prediction = torch.argmax(output, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(f\"output: {output}\\n\")\n",
    "print(f\"prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is a sequence of **probability distributions** over the possible phonemes in `vocab` **for each timestep**.  \n",
    "\n",
    "### **Formally:**  \n",
    "- Let $T$ be the number of timesteps in the model‚Äôs output (depending on the audio duration and stride).  \n",
    "- Let $N$ be the number of options in the dictionary.\n",
    "- At each timestep $t$ of the processed audio, the model predicts a probability distribution $P(l | x_t)$ over all possible phonemes $l$.  \n",
    "\n",
    "This means that for **each timestep $t$**, the model gives a **vector of size** $N$ containing the probabilities for **each possible phoneme**:\n",
    "\n",
    "$$P(l | x_t) = \\begin{bmatrix} \n",
    "P(l_1 | x_t) \\\\ \n",
    "P(l_2 | x_t) \\\\ \n",
    "\\vdots \\\\ \n",
    "P(l_N | x_t)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "In practice, many models use **log-softmax** to output the **log-probabilities** instead of raw probabilities. This is the case for this model. The log-softmax transformation applies the logarithm to the output of the softmax function:\n",
    "$$\\text{LogSoftmax}(z_{t,i}) = \\log\\left(\\frac{e^{z_{t,i}}}{\\sum_{j=1}^{N} e^{z_{t,j}}}\\right) = z_{t,i} - \\log\\left(\\sum_{j=1}^{N} e^{z_{t,j}}\\right)$$\n",
    "\n",
    "It is more often used for:\n",
    "- **Numerical stability**: Working with log-likelihoods (log-probabilities) helps avoid numerical issues with very small probability values. Directly computing probabilities (especially when values are close to 0) can lead to underflow, whereas log-probabilities prevent this.\n",
    "- **Efficiency in loss calculation**: Loss functions are typically more stable and efficient when they operate on log-likelihoods instead of raw probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "ce_loss(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>An error occured. Can you guess why? (click to expand)</b></summary>\n",
    "\n",
    "\n",
    "In ASR, the model‚Äôs output is based on **time-dependent** audio features, whereas the target is a **textual representation** of spoken words. The model produces a sequence of predictions at a high frame rate (e.g., every 30ms), while the target transcript consists of a much shorter sequence of phonemes. So, in most cases he length of the model's output sequence does not match the length of the target transcript. It is thus **not possible to directly apply cross-entropy loss** at each time step, as there is no one-to-one alignment between the model‚Äôs predictions and the ground truth labels. Instead, the model must learn how to **map variable-length sequences of audio frames to a much shorter sequence of text**: thanks to a specialized loss function, such as **Connectionist Temporal Classification (CTC) loss**, which allows the model to learn an optimal alignment between input frames and target sequences without requiring explicit per-frame supervision.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss = nn.CTCLoss(blank=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ctc_loss(\n",
    "    output.permute(1, 0, 2),\n",
    "    target,\n",
    "    input_length,\n",
    "    target_length\n",
    ")\n",
    "\n",
    "print(f'Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CTC** loss is a complex but very interesting mathematically tool defined to handle the alignment problem in ASR:\n",
    "\n",
    "#### **1. Probability of an alignment $P(\\pi | x)$**  \n",
    "Given an input sequence of length $T$ and a target transcript of length $U$ , the ASR model produces a probability distribution over possible output symbols (including a special blank symbol $\\varnothing$) at each time step.    \n",
    "The blank token ($\\varnothing$)  is a special symbol used by the CTC loss function to represent \"no output\" at a given timestep. This allows the model to \"skip\" certain time steps when the network doesn't need to output a phoneme or symbol. \n",
    "\n",
    "\n",
    "For a given alignment $\\pi = (\\pi_1, \\pi_2, ..., \\pi_T)$, the probability of this alignment given the input $x$ is:  \n",
    "\n",
    "$$P(\\pi | x) = \\prod_{t=1}^{T} P(\\pi_t | x_t)$$\n",
    "\n",
    "where $P(\\pi_t | x_t)$ is the probability of emitting symbol $\\pi_t$ at time step $t$, predicted by the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Marginalizing over all possible alignments**  \n",
    "The key idea of CTC is that multiple alignments $\\pi$ can correspond to the same final target transcript $y$.   \n",
    "For example, the word \"hello\" can be stretched across multiple time steps in different ways:  \n",
    "\n",
    "- $h\\varnothing e\\varnothing l\\varnothing l o$  \n",
    "- $hhh\\varnothing eee lll lll ooo$  \n",
    "\n",
    "Since explicit alignments are not given in the training data, all valid alignments $\\pi$ that can generate the target sequence $y$ are summed:\n",
    "\n",
    "$$P(y | x) = \\sum_{\\pi \\in \\mathcal{A}(y, T)} P(\\pi | x)$$\n",
    "\n",
    "where $\\mathcal{A}(y, T)$ is the set of all possible alignments of $y$ within $T$ time steps.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. CTC loss function**  \n",
    "The CTC loss function is defined as the **negative log-likelihood** of the correct transcript $y$ given the input sequence $x$:\n",
    "\n",
    "$$\\mathcal{L}_{CTC} = - \\log P(y | x)$$\n",
    "\n",
    "Since computing this sum over all possible alignments directly is intractable, $P(y | x)$ is computed efficiently using a **dynamic programming algorithm** similar to the **forward-backward algorithm** used in Hidden Markov Models (HMMs).\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Forward-Backward recursion for efficient computation**  \n",
    "Define $\\alpha_t(s)$ as the probability of reaching the first $s$ symbols of the target sequence by time $t$. The recursion formula for the forward variable is:\n",
    "\n",
    "$$\\alpha_t(s) = P(x_t | \\pi_t) \\cdot (\\alpha_{t-1}(s) + \\alpha_{t-1}(s-1) + \\mathbb{1}_{\\pi_s \\neq \\varnothing} \\cdot \\alpha_{t-1}(s-2))$$\n",
    "\n",
    "This efficiently accumulates probabilities over all possible alignments.\n",
    "\n",
    "\n",
    "\n",
    "*Note:* \n",
    "+ The blank token is defined as 0 in the notebook: `blank=0`   \n",
    "+ A reduction is applied to the loss: `reduction='mean'`.   \n",
    "By default, PyTorch provides three options for `reduction`:  \n",
    "  1. **`'none'`** ‚Üí Returns the loss for each sample **without aggregation**.  \n",
    "  2. **`'sum'`** ‚Üí Returns the **sum** of all sample losses.  \n",
    "  3. **`'mean'`** ‚Üí Returns the **average** loss, **divided by the total number of targets** in the batch.\n",
    "\n",
    "When using `reduction='mean'`, PyTorch **normalizes the loss based on the number of phonemes/characters**, making training more stable. As the batch has variable-length sequences, it is usually the **best choice** to avoid bias toward longer or shorter sequences.\n",
    "\n",
    "\n",
    "Get more information about CTCLoss [here](https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/43908.pdf) and about the Pytorch CTCLoss [here](https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "‚ö†Ô∏è In the ctc loss function, it is mandatory to fill the input and target lenghts **without padding**:\n",
    "\n",
    "</div>\n",
    "\n",
    "```python\n",
    "    input_lengths = torch.tensor([torch.count_nonzero(mfcc, axis=0)[0] for mfcc in mfccs], dtype=torch.long)\n",
    "    target_lengths = torch.tensor([len(seq[seq < 99]) for seq in target], dtype=torch.long)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "‚ö†Ô∏è Moreover, it is **impossible** to have `input_length < target_length`. This will result in an **error** when calculating the loss, because the CTC loss works by **learning the alignment** between the input (MFCC) and the target (transcription).   \n",
    "So it must have **at least as many temporal units (`input_length`) as the number of characters (`target_length`)**, otherwise it will not be able to generate the target sequence.   \n",
    "That is why in the `process_audio_and_text` function, there was this condition:\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "```python\n",
    "    # The number of audio feature frames must be greater than or equal to the number of phoneme labels...\n",
    "    if (features.shape[0]) < (len(encoded_phonemes)):\n",
    "        print(f\"Skipping entry otherwise CTC will be infinite...\")\n",
    "        return \n",
    "    else:\n",
    "        X.append(features)\n",
    "        Y.append(encoded_phonemes)\n",
    "```\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Now, it is time for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, the optimizer used is the asynchronous stochastic gradient descent (ASGD):   \n",
    "> We train the models in a distributed manner using asynchronous stochastic gradient descent (ASGD) optimization technique allowing parallelization of training over a large number of machines on a cluster and enabling large scale training of neural networks.   \n",
    "\n",
    "In this notebook, as a single machine was used for the training, Adam was chosen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 5e-3\n",
    "max_epochs = 10000\n",
    "epoch = 0\n",
    "verbose = True\n",
    "\n",
    "model.train()\n",
    "loss_hist = []\n",
    "batch_size = 1\n",
    "\n",
    "while True:\n",
    "    # Reset hidden LSTM states\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input, input_length)\n",
    "    predictions = torch.argmax(output, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    loss = ctc_loss(output.permute(1, 0, 2), target, input_length, target_length)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_hist.append(loss.item())\n",
    "    \n",
    "    epoch += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(predictions)\n",
    "        \n",
    "    if loss.item() < tolerance or epoch >= max_epochs:\n",
    "        print(f\"\\nConvergence reached at epoch {epoch} with a loss of {loss.item():.6f}\")\n",
    "        torch.save(model.state_dict(), \"model/single.pth\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22, 8))\n",
    "plt.plot(loss_hist, color='blue', linestyle='-', marker='o')\n",
    "plt.title('Learning rate evolution')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(input, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(output.shape[1]):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(output.shape[2])),\n",
    "        y=output[0, i, :].detach().numpy(),\n",
    "        mode='lines',\n",
    "        name=f'{i}-th timestamp'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Logits for each timestamp',\n",
    "    xaxis_title='Phoneme number',\n",
    "    yaxis_title='Log-probability',\n",
    "    legend_title='Timestep'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows the probabilities (or more precisely, the log-probabilities) of the module's predictions for each output in the vocabulary.   \n",
    "There are very sharp peaks, which means that for each timestamp, the model is confident in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.argmax(output, dim=-1)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_phonemes(encoded_sequence, vocab):\n",
    "    # Invert the dictionary to obtain an index ‚Üí phoneme mapping\n",
    "    inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Decoding the numerical sequence into phonemes\n",
    "    decoded_phonemes = [inverse_vocab.get(idx) for idx in encoded_sequence]\n",
    "    \n",
    "    return decoded_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_phonemes = decode_phonemes(prediction.squeeze(0).detach().numpy(), vocab)\n",
    "print(f\"model's output: {decoded_phonemes}\")\n",
    "print(f\"target: {decode_phonemes(target.squeeze(0).detach().numpy(), vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model on a **single audio sample** for more than **150 epochs**, the prediction closely matches the target with high accuracy. However, the alignment is not necessarily identical, as the same phoneme can appear across multiple timesteps, leading to some **repetitions** in the output. This behavior is expected due to the nature of sequence modeling in ASR, where **different alignments can still yield the correct transcription**. Despite these variations, the result is highly **satisfying**, demonstrating that the model has successfully learned the mapping from audio features to phonemes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "In the following cell, **you can try to train the model with your own vocal**. But please, make sure:\n",
    "- The vocal is is in english.\n",
    "- The vocal is \"longer\" than the transcript:\n",
    "</div>\n",
    "\n",
    "  ```python \n",
    "    (vocal.shape[0] > len(transcript))\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 5e-3\n",
    "max_epochs = 10000\n",
    "verbose = True\n",
    "batch_size = 1\n",
    "\n",
    "def train_single_audio(audio, transcript):\n",
    "    # Extract input features\n",
    "    y, sr = librosa.load(audio, sr=16000)\n",
    "    features = normalize_mfccs(stack_decimate_frames(extract_features(y)))\n",
    "    input_tensor = torch.as_tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "    input_length = torch.tensor([input_tensor.shape[1]], dtype=torch.long)\n",
    "\n",
    "\n",
    "    # Encode target\n",
    "    target = np.array(encode_phonemes(transcript, vocab))\n",
    "    target_length = torch.tensor([len(target)], dtype=torch.long)\n",
    "    target = torch.tensor(target, dtype=torch.int32).unsqueeze(0)\n",
    "\n",
    "    if input_length < target_length:\n",
    "        raise ValueError(\"Enable to process this audio!\\nTarget must be shorter!\")\n",
    "        return\n",
    "    \n",
    "    epoch = 0\n",
    "    while True:\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input, input_length)\n",
    "        predictions = torch.argmax(output, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "        loss = ctc_loss(output.permute(1, 0, 2), target, input_length, target_length)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(predictions)\n",
    "            \n",
    "        if loss.item() < tolerance or epoch >= max_epochs:\n",
    "            print(f\"\\nConvergence reached at epoch {epoch} with a loss of {loss.item():.6f}\")\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT\n",
    "\n",
    "audio = '....mp3'\n",
    "transcript = '...'\n",
    "model = SpeechRecognitionModel(input_dim, hidden_dim, num_layers, num_classes)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "# model.train()\n",
    "\n",
    "# train_single_audio(audio, transcript)\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# output = model(input, input_length)\n",
    "# predictions = torch.argmax(output, dim=-1)\n",
    "# decoded_phonemes = decode_phonemes(predictions.squeeze(0).detach().numpy(), vocab)\n",
    "# print(f\"model's output: {decoded_phonemes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step is to **generalize** the model to handle multiple audio samples instead of just one.  \n",
    "Training on a single sample allows the model to **overfit** to that specific example, but for real-world ASR, it needs to learn a more **general representation** of speech.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create PyTorch dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\", align=\"center\">\n",
    "\n",
    "The following cell is the training cell. You can uncomment and run it if you want, but to avoid losing time, weights of trained models are stored in the `model/` folder.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "num_epochs = 75\n",
    "batch_size = 8\n",
    "\n",
    "model = SpeechRecognitionModel(input_dim, hidden_dim, num_layers, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5) \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "# model.train()\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# lrs = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_train_loss = 0\n",
    "\n",
    "#     for i, (X, y) in enumerate(train_dataloader):\n",
    "#         mfccs, targets = X.to(device), y.to(device)\n",
    "\n",
    "#         input_length = torch.tensor([torch.count_nonzero(mfcc, axis=0)[0] for mfcc in mfccs], dtype=torch.long).to(device)\n",
    "#         target_length = torch.tensor([len(seq[seq < 99]) for seq in targets], dtype=torch.long).to(device)\n",
    "\n",
    "#         hidden = model.init_hidden(batch_size)\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         outputs = model(mfccs, input_length)\n",
    "\n",
    "#         loss = ctc_loss(\n",
    "#             outputs.permute(1, 0, 2), # (T, N, C)\n",
    "#             targets,\n",
    "#             input_length,\n",
    "#             target_length\n",
    "#         )\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Limiting memory cell gradients to [-1, 1].\n",
    "#         for param in model.lstm.parameters():\n",
    "#             if param.grad is not None:\n",
    "#                 param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_train_loss += loss.item()\n",
    "#         print(f\"Batch {i + 1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "#     avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "#     train_losses.append(avg_train_loss)\n",
    "\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for i, (X, y) in enumerate(test_dataloader):\n",
    "#             mfccs, targets = X.to(device), y.to(device)\n",
    "\n",
    "#             input_length = torch.tensor([torch.count_nonzero(mfcc, axis=0)[0] for mfcc in mfccs], dtype=torch.long).to(device)\n",
    "#             target_length = torch.tensor([len(seq[seq < 99]) for seq in targets], dtype=torch.long).to(device)\n",
    "        \n",
    "#             outputs = model(mfccs, input_length)\n",
    "\n",
    "#             loss = ctc_loss(\n",
    "#                 outputs.permute(1, 0, 2),\n",
    "#                 targets,\n",
    "#                 input_length,\n",
    "#                 target_length\n",
    "#             )\n",
    "\n",
    "#             total_val_loss += loss.item()\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(test_dataloader)\n",
    "#     lrs.append(optimizer.param_groups[0]['lr'])\n",
    "#     scheduler.step(avg_val_loss) \n",
    "#     val_losses.append(avg_val_loss)\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the evolution of the loss for the trained model:\n",
    "<h3 align='center'>\n",
    "    <img src=\"img/loss.png\">\n",
    "</h3>\n",
    "\n",
    "<br>\n",
    "\n",
    "And the evolution of its learning rate: \n",
    "<h3 align='center'>\n",
    "    <img src=\"img/lr.png\">\n",
    "</h3>\n",
    "\n",
    "<br>\n",
    "\n",
    "It is noteworthy that the training loss decreases, but the validation loss doesn't really decrease, despite the techniques used to prevent overfitting, such as the scheduler, dropout, and weight decay. Starting from epoch 50, the scheduler reduces the learning rate to 0, and the model no longer improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model/model_weights_augmented.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper also recommends to clip the gradients of memory cells:\n",
    "> We clip the activations of memory cells to [-50, 50], and their gradients to [-1, 1], making CTC training stable.\n",
    "\n",
    "That is why in the training loop there is this code to limit the norm of the gradients:\n",
    "```python\n",
    "    loss.backward()\n",
    "\n",
    "    # Limiting memory cell gradients to [-1, 1].\n",
    "    for param in model.lstm.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Now, it is time to test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"model/model_weights_augmented.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "saved_model = SpeechRecognitionModel(input_dim, hidden_dim, num_layers, num_classes)\n",
    "saved_model.load_state_dict(torch.load(path_to_model, weights_only=True))\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following audio and transcript will be used for the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_test = 'data/audio_sample/common_voice_en_38614407.mp3' \n",
    "transcript = \"I'm in excellent health.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(audio_test, sr=16000)\n",
    "features = normalize_mfccs(stack_decimate_frames(extract_features(y)))\n",
    "input = torch.as_tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "input_length = torch.tensor([input.shape[1]], dtype=torch.long)\n",
    "\n",
    "encoded_phonemes = encode_phonemes(transcript, vocab)\n",
    "target = np.array(encoded_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = saved_model(input, input_length)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(prediction.shape[1]):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(prediction.shape[2])),\n",
    "        y=prediction[0, i, :].detach().numpy(),\n",
    "        mode='lines',\n",
    "        name=f'{i}-th timestamp'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Logits for each timestamp',\n",
    "    xaxis_title='Phoneme number',\n",
    "    yaxis_title='Log-probability',\n",
    "    legend_title='Timestep'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the graph, the peaks are much less sharp than when the model was trained on a single audio. It indicates **high uncertainty** in the model's prediction. Ideally, a well-trained ASR model should assign a high probability to a single phoneme (or blank token in CTC) at each timestep.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_prediction = torch.argmax(prediction, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_prediction = decode_phonemes(encoded_prediction.flatten().detach().cpu().numpy(), vocab)\n",
    "\n",
    "print(f\"Encoded predictions: {encoded_prediction}\")\n",
    "print(f\"Decoded predictions: {decoded_prediction}\")\n",
    "\n",
    "\n",
    "decoded_target_phonemes = decode_phonemes(target, vocab)\n",
    "print(f\"Encoded target phonemes: {target}\")\n",
    "print(f\"Decoded target phonemes: {decoded_target_phonemes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is very mediocre and this is because, the modele is mediocre. It still has trouble generalizing despite the addition of `dropout`, `weight decay`, and a `scheluder` that adjusts the learning rate because:\n",
    "\n",
    "- Insufficient or non-representative data:\n",
    "The dataset is probably not **big** enough, the model can **learn specific patterns** instead of generalizing. But the current dataset contains 3000 audios and it took already a lot of time to process.\n",
    "\n",
    "- Use of LSTMs\n",
    "    Today Transformers have replaced LSTMs in Speech Recognition. Here's why:\n",
    "\n",
    "    | **Criterion** | **LSTMs** | **Transformers** |\n",
    "    |--------------------|--------------------------------|--------------------------------|\n",
    "    | **Long-term dependencies** | Memory problem on long sequences (vanishing gradient) | Learns on the entire sequence at once thanks to **attention** |\n",
    "    | **Parallelization** | Sequential ‚Üí slow to train | Can train **much faster** thanks to parallelism |\n",
    "    | **Captures global relations** | Depends only on previous hidden states | Captures **global** relations between phonemes and words |\n",
    "    | **Adapts to noisy data** | Sensitive to noise (because it depends too much on the past) | More robust thanks to **self-attention layers** |\n",
    "    | **Performance on large datasets** | Can saturate when too much data | Improves with more data (scalability) |\n",
    "\n",
    "    Concretely:\n",
    "    1. **LSTMs are less efficient at learning long dependencies**:   \n",
    "        In speech recognition, some relationships **between phonemes are distant**, so an LSTM may have trouble capturing this information well.\n",
    "\n",
    "    2. **Transformers (Ex: Wav2Vec2, Conformer) are better for Speech-to-Text**:  \n",
    "        Today, **almost all state-of-the-art models** in speech recognition are based on Transformers (ex: **[Whisper](https://openai.com/index/whisper/), [Wav2Vec2](https://ai.meta.com/research/impact/wav2vec/), [Conformer](https://research.google/pubs/conformer-convolution-augmented-transformer-for-speech-recognition/)**). \n",
    "\n",
    "    3. **LSTMs are sequential**:   \n",
    "        Training cannot be parallelized efficiently, so **Transformers learn faster** with more data.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The paper was realized by a *Google* team, which has obviously better datasets, better infrastructure and computing power to train a model for a long time.   \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### For the last part of the notebook, the model that learned the sentence \"Jesus loves me, he who died\" perfectly will be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"model/single.pth\"\n",
    "saved_model = SpeechRecognitionModel(input_dim, hidden_dim, num_layers, num_classes)\n",
    "saved_model.load_state_dict(torch.load(path_to_model, weights_only=True))\n",
    "saved_model.eval()\n",
    "\n",
    "\n",
    "audio_test = 'data/audio_sample/common_voice_en_39177958.mp3' \n",
    "transcript = 'Jesus loves me, he who died.'\n",
    "\n",
    "# Extract input features\n",
    "y, sr = librosa.load(audio_test, sr=16000)\n",
    "features = normalize_mfccs(stack_decimate_frames(extract_features(y)))\n",
    "input = torch.as_tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "input_length = torch.tensor([input.shape[1]], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Encode target\n",
    "target = np.array(encode_phonemes(transcript, vocab))\n",
    "target_length = torch.tensor([len(target)], dtype=torch.long)\n",
    "target = torch.tensor(target, dtype=torch.int32).unsqueeze(0)\n",
    "output = saved_model(input, input_length)\n",
    "prediction = torch.argmax(output, dim=-1).squeeze(0)\n",
    "decoded_prediction = decode_phonemes(prediction.detach().cpu().numpy(), vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective has almost been achieved, all that's left to do is to convert the phoneme sequences into text.\n",
    "\n",
    "\n",
    "## IV. From phonemes to text\n",
    "\n",
    "\n",
    "Once the model generates predictions and decodes them into phonemes, the next challenge is converting the sequence of phonemes into human-readable text. This process involves **decoding** the phoneme sequence into actual words. This task is not straightforward, as prediction errors are possible. Therefore, it is crucial to decode a coherent and contextually accurate transcription from the sequence of phonemes. Several techniques can be employed to achieve this:\n",
    "\n",
    "- **greedy decoding**, where the most likely phoneme is chosen at each timestep, resulting in a straightforward, but potentially suboptimal, transcription. While this method is simple, it may not always handle ambiguities or multiple possible transcriptions well. \n",
    "- **beam search decoding**, which explores multiple possible sequences of phonemes by maintaining a \"beam\" of the top \\( k \\) most probable sequences at each timestep. This approach allows for better handling of uncertainty in the model's predictions, ultimately producing more accurate transcriptions.\n",
    "- **language models**: Some systems use LM in conjunction with the phoneme decoder to improve performance. A language model helps to filter out improbable phoneme sequences by assigning higher probabilities to sequences that conform to natural language rules. This post-processing step can be particularly useful when the phoneme sequence alone might lead to nonsensical or ambiguous outputs.\n",
    "- **CTC decoding**: These methods are often used in speech recognition systems, especially in the context of models trained with Connectionist Temporal Classification (CTC). In this case, the CTC loss function itself implicitly helps the model find the best alignment between audio and transcript, but a dedicated decoding algorithm like **CTC beam search** is needed to handle the blank symbols and to find the optimal transcription.\n",
    "\n",
    "For more information about context dependency, here is a [paper](https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/43910.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the evaluation of ASR (Automatic Speech Recognition) models, one of the most commonly used metrics is the **Word Error Rate (WER)**. This metric provides an indication of the accuracy of the model's transcriptions by comparing the predicted text to the reference transcription.\n",
    "\n",
    "The **WER** is calculated based on the number of insertions, deletions, and substitutions required to transform the predicted transcript into the correct reference transcript. Specifically, it is computed as:\n",
    "\n",
    "$$WER = \\frac{S + D + I}{N}$$\n",
    "\n",
    "where:\n",
    "- $S$: the number of substitutions (incorrect words).\n",
    "- $D$: the number of deletions (missing words).\n",
    "- $I$: the number of insertions (extra words). \n",
    "- $N$: the total number of words in the reference transcript.\n",
    "\n",
    "A **lower WER** indicates better performance, as it reflects fewer errors in the predicted transcription. The metric is widely used because it directly reflects the quality of transcriptions, and it is simple to compute. However, while WER is useful for quantifying overall accuracy, it does not always capture the nuances of word context or phonetic correctness, which can be important in certain applications of ASR systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer(target_transcript, predicted_transcript):\n",
    "    # Tokenize transcripts into words\n",
    "    target_words = target_transcript.split()\n",
    "    predicted_words = predicted_transcript.split()\n",
    "\n",
    "    len_target = len(target_words)\n",
    "    len_predicted = len(predicted_words)\n",
    "\n",
    "    # Handle edge case: Empty target transcript\n",
    "    if len_target == 0:\n",
    "        return 1.0 if len_predicted > 0 else 0.0  # If target is empty but prediction isn't, WER = 100%\n",
    "\n",
    "    # Initialize DP matrix\n",
    "    dp = [[0] * (len_predicted + 1) for _ in range(len_target + 1)]\n",
    "\n",
    "    # Fill base cases\n",
    "    for i in range(len_target + 1):\n",
    "        dp[i][0] = i  # Deletions\n",
    "    for j in range(len_predicted + 1):\n",
    "        dp[0][j] = j  # Insertions\n",
    "\n",
    "    # Compute WER using dynamic programming\n",
    "    for i in range(1, len_target + 1):\n",
    "        for j in range(1, len_predicted + 1):\n",
    "            if target_words[i - 1] == predicted_words[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]  # No operation needed\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i - 1][j - 1],  # Substitution\n",
    "                               dp[i - 1][j],      # Deletion\n",
    "                               dp[i][j - 1]) + 1  # Insertion\n",
    "\n",
    "    # Compute final WER\n",
    "    wer = dp[len_target][len_predicted] / len_target\n",
    "    return wer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Greedy decoder\n",
    "Greedy search is the simplest decoding method. It selects the word with the highest probability as its next word: $w_t=argmax_w P(w‚à£w_{1:t‚àí1})$ at each timestep $t$. The following sketch shows greedy search.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "An greedy algorithm based on **CMUdict (Carnegie Mellon Pronouncing Dictionary)**, was designed. The [CMUdict](https://en.wikipedia.org/wiki/CMU_Pronouncing_Dictionary) is a **phonetic dictionary** for **American English**, developed by the **Carnegie Mellon University Speech Group**. It provides **word-to-phoneme mappings**, where each word is transcribed in terms of its **phonemes using the ARPAbet notation**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('cmudict')\n",
    "\n",
    "# Download the CMU Pronouncing Dictionary\n",
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "# Inverse dictionary {tuple(phonemes) -> word}\n",
    "phoneme_to_word = {}\n",
    "for word, pron_list in cmu_dict.items():\n",
    "    clean_pron = tuple(p.rstrip('012') for p in pron_list[0])\n",
    "    phoneme_to_word[clean_pron] = word.lower()\n",
    "\n",
    "def phonemes_to_text_greedy(phonemes):\n",
    "\n",
    "    phonemes_clean = [char for char in phonemes if char != '-'] \n",
    "\n",
    "    result_words = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(phonemes_clean):\n",
    "        found = False\n",
    "        # Try to find the longest word possible\n",
    "        for end in range(len(phonemes_clean), start, -1):\n",
    "            sub_seq = tuple(phonemes_clean[start:end])\n",
    "            if sub_seq in phoneme_to_word:\n",
    "                result_words.append(phoneme_to_word[sub_seq])\n",
    "                start = end  # Move on to the next phonemes\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            start += 1  # Go forward one phoneme to try again\n",
    "\n",
    "    return \" \".join(result_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is necessary to process the prediction. in fact, as the model can provide multiple times the same output if the phoneme stretches on multiple timestamps. Thus, consecutive equal phonemes must be deleted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_consecutive_duplicates(lst):\n",
    "    result = []\n",
    "    for i in range(len(lst)):\n",
    "        if i == 0 or lst[i] != lst[i-1]:\n",
    "            result.append(lst[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_prediction_processed = remove_consecutive_duplicates(decoded_prediction)\n",
    "print(f\"Model ouput: {decoded_prediction_processed}\")\n",
    "greedy_output = phonemes_to_text_greedy(decoded_prediction_processed)\n",
    "print(f\"Greedy algorithm ouput: {greedy_output}\")\n",
    "print(f\"Target: {transcript}\")\n",
    "print(f\"WER: {compute_wer(transcript, greedy_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WER here is equal to 1, because there is an error for every word. The greedy decoder is pretty mediocre!\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### 2. Beam search decoder   \n",
    "Unlike a greedy decoder, which selects the most probable phoneme sequence at each timestep, **beam search** considers multiple possible sequences and keeps the most probable ones. This helps **correct errors** and **produce more coherent words**.   \n",
    "\n",
    "**Beam width** (also called **beam size**) is a key **hyperparameter** in **beam search decoding**. It determines **how many candidate sequences** are kept at each step: \n",
    "- At each step, the decoder **keeps only the top-k most probable sequences**.\n",
    "- A larger beam width allows the decoder to explore **more possibilities**, improving accuracy but increasing computational cost.\n",
    "(If beam width = 1, the decoder chooses the most probable word at each step ‚Üí Greedy decoding (fast but often inaccurate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A beam search takes much longer than a greedy search but is much more accurate. This is why beam search is not used in ASR systems, where a rapid response is essential. Instead, language models (LMs) are used.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 3. LM\n",
    "\n",
    "[KenLM](https://github.com/kpu/kenlm) is a **fast and efficient n-gram language model** widely used in speech recognition and NLP tasks. It helps in decoding by ranking possible word sequences based on their probability in natural language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
